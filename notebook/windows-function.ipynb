{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from etl_project.commons import utils\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.window import Window\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_data = \\\n",
    "[('Alex','2018-10-10','Paint',80),('Alex','2018-04-02','Ladder',20),('Alex','2018-06-22','Stool',20),\\\n",
    "('Alex','2018-12-09','Vacuum',40),('Alex','2018-07-12','Bucket',5),('Alex','2018-02-18','Gloves',5),\\\n",
    "('Alex','2018-03-03','Brushes',30),('Alex','2018-09-26','Sandpaper',10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session, log = utils.start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.createDataFrame(shopping_data, ['name','date','product','price'])\\\n",
    "                .withColumn('date',F.col('date').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = Window.partitionBy('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Window [rank(price#19L) windowspecdefinition(name#16, price#19L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS price_rank#66], [name#16], [price#19L ASC NULLS FIRST]\n",
      "+- *(2) Sort [name#16 ASC NULLS FIRST, price#19L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(name#16, 200), true, [id=#48]\n",
      "      +- *(1) Project [name#16, cast(date#17 as date) AS date#24, product#18, price#19L]\n",
      "         +- *(1) Scan ExistingRDD[name#16,date#17,product#18,price#19L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('price_rank',F.rank().over(w0.orderBy(F.col('price').asc()))).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('price_rank',F.dense_rank().over(w0.orderBy(F.col('price').desc()))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('avg_to_date',     F.round(F.avg('price').over(w0.orderBy(F.col('date'))),2))\\\n",
    "  .withColumn('accumulating_sum',F.sum('price').over(w0.orderBy(F.col('date'))))\\\n",
    "  .withColumn('max_to_date',     F.max('price').over(w0.orderBy(F.col('date'))))\\\n",
    "  .withColumn('max_of_last2',    F.max('price').over(w0.orderBy(F.col('date')).rowsBetween(-1,Window.currentRow)))\\\n",
    "  .withColumn('items_to_date',   F.count('*').over(w0.orderBy(F.col('date'))))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('days_from_last_purchase', F.datediff('date',F.lag('date',1).over(w0.orderBy(F.col('date')))))\\\n",
    "  .withColumn('days_before_next_purchase', F.datediff(F.lead('date',1).over(w0.orderBy(F.col('date'))),'date'))\\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-sample",
   "language": "python",
   "name": "pyspark-sample"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
