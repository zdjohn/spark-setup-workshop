{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOB 2\n",
    "\n",
    "we are going look into:\n",
    "    \n",
    "    - join\n",
    "    - get unique values from selected column\n",
    "    - light touch on window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import findspark; findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import configparser\n",
    "from os import environ, listdir, path\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from src.commons import utils\n",
    "from src.cross_domain_reviews import etl\n",
    "from src.amazon_reviews.etl import to_stats_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR 6.10, as spark is based on JVM, version number matters\n",
    "environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=com.amazonaws:aws-java-sdk:1.11.900,org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\"\n",
    "environ['DEBUG'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session, logger= utils.start_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read the cleaned review datasets from S3, which is carried out in `amazon_reviews` job. (see source code in https://github.com/zdjohn/spark-setup-workshop/tree/master/src/amazon_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = utils.extract_parquet_data(session, etl.DIG_MUSIC)\n",
    "video_df = utils.extract_parquet_data(session, etl.DIG_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "music_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|users_count|items_count|\n",
      "+-----------+-----------+\n",
      "|     144240|     401484|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "music_df.agg(\n",
    "    F.countDistinct('customer_id').alias('users_count')\n",
    "    , F.countDistinct('product_id').alias('items_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|users_count|items_count|\n",
      "+-----------+-----------+\n",
      "|     501158|     127873|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_df.agg(\n",
    "    F.countDistinct('customer_id').alias('users_count')\n",
    "    , F.countDistinct('product_id').alias('items_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make video recommendations based on the user-item interaction information we learned from music. \n",
    "(google \"transfer learning\" if you like to know more) \n",
    "\n",
    "as a result, we need to find overlapping users who purchased both video and music product for model to learn the corelation between `video` and `music` domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20582"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_corss_reviews = etl.to_overlapping_customers(music_df, video_df)\n",
    "music_corss_reviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see there are **20582** customers out of (144240 music product customer, 501158 video customer) have done shopping on both amazon music and video products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(product_id)|\n",
      "+-----------------+\n",
      "|            96198|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corss_music_products = etl.to_overlapping_reviews(music_df,music_corss_reviews)\n",
    "corss_music_products.agg(F.countDistinct('product_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(product_id)|\n",
      "+-----------------+\n",
      "|            41764|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corss_video_products = etl.to_overlapping_reviews(video_df,music_corss_reviews)\n",
    "corss_video_products.agg(F.countDistinct('product_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit job from your local machine\n",
    "\n",
    "run command tox -e pack releasable artifact will be generated inside ./dist folder.\n",
    "```\n",
    "dist/\n",
    "  ├── dist_files.zip\n",
    "  └── main.py\n",
    "```  \n",
    "\n",
    "step into dist folder involke spark submit:\n",
    "\n",
    "```\n",
    "$: spark-submit \\\n",
    "    --master local[3] \\\n",
    "    --deploy-mode client \\\n",
    "    --packages=com.amazonaws:aws-java-sdk:1.11.900,org.apache.hadoop:hadoop-aws:3.2.0 \\\n",
    "    --py-files ./dist_files.zip \\\n",
    "    main.py --job=cross_domain --source_domain=music --target_domain=video --local_run=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-sample",
   "language": "python",
   "name": "pyspark-sample"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
