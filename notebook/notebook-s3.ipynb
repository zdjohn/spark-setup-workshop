{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import findspark; findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import configparser\n",
    "from os import environ, listdir, path\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from etl_project.commons import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR 6.10\n",
    "environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=com.amazonaws:aws-java-sdk:1.11.900,org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environ['DEBUG'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get('default', \"aws_access_key_id\") \n",
    "access_key = config.get('default', \"aws_secret_access_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session, logger= utils.start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hadoop.apache.org/docs/current3/hadoop-aws/tools/hadoop-aws/s3n.html#How_to_migrate_to_to_the_S3A_client\n",
    "hadoop_conf=session._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", access_id)\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=session.read.csv(\"s3a://covid19-lake/static-datasets/csv/state-abv/states_abv.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"s3a://pyspark3-sample/states.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-sample",
   "language": "python",
   "name": "pyspark-sample"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
